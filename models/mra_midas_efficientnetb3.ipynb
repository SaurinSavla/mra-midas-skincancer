{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:/Data/DJ/azcopydata/midasmultimodalimagedatasetforaibasedskincancer/release_midas.xlsx\"\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique label values: [0. 1.]\n",
      "Label data type: float64\n"
     ]
    }
   ],
   "source": [
    "image_dir = os.path.dirname(file_path)\n",
    "df = df.astype(str)\n",
    "\n",
    "label_encoders = {}\n",
    "categorical_cols = ['midas_gender', 'midas_fitzpatrick', 'midas_ethnicity', 'midas_race']\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "numerical_cols = ['midas_age', 'length_(mm)', 'width_(mm)']\n",
    "df[numerical_cols] = df[numerical_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols].fillna(0))\n",
    "\n",
    "df['target'] = df['midas_melanoma'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "df = df.dropna(subset=['target']).reset_index(drop=True)\n",
    "\n",
    "print(\"Unique label values:\", df[\"target\"].unique())\n",
    "print(\"Label data type:\", df[\"target\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exists(filename):\n",
    "    possible_paths = [\n",
    "        os.path.join(image_dir, filename),\n",
    "        os.path.join(image_dir, filename.replace('.jpg', '.jpeg')),\n",
    "        os.path.join(image_dir, filename.replace('.jpeg', '.jpg'))\n",
    "    ]\n",
    "    return any(os.path.exists(path) for path in possible_paths)\n",
    "\n",
    "df = df[df['midas_file_name'].apply(file_exists)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRAMIDASDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Load image with flexible file extension handling\n",
    "        img_filename = row['midas_file_name']\n",
    "        img_path = os.path.join(self.image_dir, img_filename)\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            # Try alternative extensions\n",
    "            img_path_jpeg = img_path.replace('.jpg', '.jpeg')\n",
    "            img_path_jpg = img_path.replace('.jpeg', '.jpg')\n",
    "            if os.path.exists(img_path_jpeg):\n",
    "                img_path = img_path_jpeg\n",
    "            elif os.path.exists(img_path_jpg):\n",
    "                img_path = img_path_jpg\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Image not found: {img_filename}\")\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_values = row[categorical_cols + numerical_cols].values.astype(float)\n",
    "        metadata = torch.tensor(metadata_values, dtype=torch.float32)\n",
    "        \n",
    "        # Ensure label is an integer before creating tensor\n",
    "        label = int(row['target'])  # Convert from potential object type\n",
    "        label = torch.tensor(label, dtype=torch.int64)\n",
    "        \n",
    "        return image, metadata, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((380, 380)),  # Increase resolution\n",
    "    transforms.RandomHorizontalFlip(p=0.7),  # Increase probability of flipping\n",
    "    transforms.RandomRotation(30),  # Higher rotation range\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),  # More aggressive color changes\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['target'].values\n",
    "class_sample_count = np.array([len(np.where(labels == t)[0]) for t in np.unique(labels)])\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = np.array([weight[int(t)] for t in labels])\n",
    "samples_weight = torch.tensor(samples_weight, dtype=torch.float)\n",
    "sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "dataset = MRAMIDASDataset(df, image_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageModel, self).__init__()\n",
    "        self.model = models.efficientnet_b3(pretrained=True)  # Use B3 instead of B0\n",
    "        self.model.classifier[1] = nn.Linear(1536, 256)  # Increase output layer size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_weights = F.softmax(self.W(x), dim=1)\n",
    "        return x * attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetadataModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MetadataModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),  # Increase FC layers size\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(128),\n",
    "            AttentionLayer(128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, image_model, metadata_model):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.image_model = image_model\n",
    "        self.metadata_model = metadata_model\n",
    "        self.classifier = nn.Linear(256 + 32, 2)  # Combining both feature sets\n",
    "    \n",
    "    def forward(self, image, metadata):\n",
    "        img_features = self.image_model(image)\n",
    "        meta_features = self.metadata_model(metadata)\n",
    "        combined = torch.cat((img_features, meta_features), dim=1)\n",
    "        return self.classifier(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91935\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\91935\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "image_model = ImageModel().to(device)\n",
    "metadata_model = MetadataModel(input_size=len(categorical_cols) + len(numerical_cols)).to(device)\n",
    "model = MultimodalModel(image_model, metadata_model).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0002, weight_decay=1e-5)  # Lower LR\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melanoma = df[df[\"target\"] == 1]\n",
    "df_non_melanoma = df[df[\"target\"] == 0]\n",
    "\n",
    "df_oversampled = pd.concat([df, df_melanoma, df_melanoma, df_melanoma])  # 3x duplication\n",
    "df_oversampled = df_oversampled.sample(frac=1).reset_index(drop=True)  # Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"C:/Data/DJ/SkinCancer/code/pytorch_models/mra_midas_efficientnetB3.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "train_precisions = []\n",
    "train_recalls = []\n",
    "train_f1_scores = []\n",
    "epoch_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device :cpu\n",
      "Started epoch - 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device :{device}\")\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()  # Start timer\n",
    "    total_loss = 0\n",
    "    correct, total = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(f\"Started epoch - {epoch+1}\")\n",
    "\n",
    "    for images, metadata, labels in dataloader:\n",
    "        images, metadata, labels = images.to(device), metadata.to(device), labels.to(torch.int64).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, metadata)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Collect predictions and labels for Precision, Recall, F1-score\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute epoch metrics\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
    "    recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "    epoch_time = time.time() - start_time  # End timer\n",
    "\n",
    "    # Store metrics\n",
    "    train_losses.append(total_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "    train_precisions.append(precision)\n",
    "    train_recalls.append(recall)\n",
    "    train_f1_scores.append(f1)\n",
    "    epoch_times.append(epoch_time)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}, Time: {epoch_time:.2f} seconds\")\n",
    "\n",
    "    # **Save checkpoint every 5 epochs**\n",
    "    if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:  # Save every 5 epochs and last epoch\n",
    "        checkpoint = {\n",
    "            'epoch': epoch+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'train_precisions': train_precisions,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'train_recalls': train_recalls,\n",
    "            'train_f1_scores': train_f1_scores,\n",
    "            'epoch_times': epoch_times\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
