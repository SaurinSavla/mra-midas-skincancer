{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchviz import make_dot\n",
    "from torchinfo import summary\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageModel, self).__init__()\n",
    "        self.model = models.efficientnet_b0(pretrained=True)  # Load pretrained EfficientNet-B0\n",
    "        self.model.classifier[1] = nn.Linear(self.model.classifier[1].in_features, 128)  # Adjust output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_weights = F.softmax(self.W(x), dim=1)\n",
    "        return x * attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetadataModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MetadataModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(128),\n",
    "            AttentionLayer(128),  # Apply attention to highlight key features\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Multimodal Model\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, image_model, metadata_model):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.image_model = image_model\n",
    "        self.metadata_model = metadata_model\n",
    "        self.classifier = nn.Linear(128 + 32, 2)  # Combining both feature sets\n",
    "    \n",
    "    def forward(self, image, metadata):\n",
    "        img_features = self.image_model(image)\n",
    "        meta_features = self.metadata_model(metadata)\n",
    "        combined = torch.cat((img_features, meta_features), dim=1)\n",
    "        return self.classifier(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91935\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\91935\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_model = ImageModel().to(device)\n",
    "metadata_model = MetadataModel(input_size=7).to(device)\n",
    "model = MultimodalModel(image_model, metadata_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91935\\AppData\\Local\\Temp\\ipykernel_29456\\4098679553.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# Load saved checkpoint\n",
    "checkpoint_path = \"C:/Data/DJ/SkinCancer/code/pytorch_models/mra_midas_efficientnetB0.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_visualization.png'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "# Get a sample input\n",
    "sample_image = torch.randn(1, 3, 224, 224).to(device)\n",
    "sample_metadata = torch.randn(1, 7).to(device)\n",
    "sample_output = model(sample_image, sample_metadata)\n",
    "\n",
    "# Generate visualization\n",
    "dot = make_dot(sample_output, params=dict(model.named_parameters()))\n",
    "dot.render(\"model_visualization\", format=\"png\")  # Save as PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_visualization_simplified-1.png'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "# Get a sample input\n",
    "sample_image = torch.randn(1, 3, 224, 224).to(device)\n",
    "sample_metadata = torch.randn(1, 7).to(device)\n",
    "\n",
    "# Extract the final layer outputs instead of the entire model\n",
    "img_features = model.image_model(sample_image)\n",
    "meta_features = model.metadata_model(sample_metadata)\n",
    "combined_features = torch.cat((img_features, meta_features), dim=1)\n",
    "final_output = model.classifier(combined_features)\n",
    "\n",
    "# Generate visualization (only for the classifier)\n",
    "dot = make_dot(final_output, params=dict(model.classifier.named_parameters()))\n",
    "dot.render(\"model_visualization_simplified-1\", format=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_visualization_simplified_colored.png'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Get a sample input\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sample_image = torch.randn(1, 3, 224, 224).to(device)\n",
    "sample_metadata = torch.randn(1, 7).to(device)\n",
    "\n",
    "# Extract features separately\n",
    "img_features = model.image_model(sample_image)\n",
    "meta_features = model.metadata_model(sample_metadata)\n",
    "\n",
    "# Generate individual graphs\n",
    "dot_img = make_dot(img_features, params=dict(model.image_model.named_parameters()))\n",
    "dot_meta = make_dot(meta_features, params=dict(model.metadata_model.named_parameters()))\n",
    "\n",
    "# Assign colors\n",
    "for node in dot_img.body:\n",
    "    node = node.replace('fillcolor=black', 'fillcolor=lightblue')\n",
    "\n",
    "for node in dot_meta.body:\n",
    "    node = node.replace('fillcolor=black', 'fillcolor=lightgreen')\n",
    "\n",
    "# Combine features\n",
    "combined_features = torch.cat((img_features, meta_features), dim=1)\n",
    "final_output = model.classifier(combined_features)\n",
    "\n",
    "# Generate final visualization\n",
    "dot_final = make_dot(final_output, params=dict(model.classifier.named_parameters()))\n",
    "\n",
    "# Merge color changes\n",
    "dot_final.body.extend(dot_img.body)\n",
    "dot_final.body.extend(dot_meta.body)\n",
    "\n",
    "# Save and render\n",
    "dot_final.render(\"model_visualization_simplified_colored\", format=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_visualization_small-2.png'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in model.image_model.model.features.parameters():\n",
    "    param.requires_grad = False  # Freeze feature extractor\n",
    "\n",
    "dot = make_dot(sample_output, params=dict(model.classifier.named_parameters()))\n",
    "dot.render(\"model_visualization_small-2\", format=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_visualization_filtered-3.png'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = make_dot(sample_output, params={name: p for name, p in model.named_parameters() if 'classifier' in name or 'fc' in name})\n",
    "dot.render(\"model_visualization_filtered-3\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\91935\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.20.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting netron\n",
      "  Downloading netron-8.1.9-py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading netron-8.1.9-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.9 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/1.9 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.3/1.9 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/1.9 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: netron\n",
      "Successfully installed netron-8.1.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!netron model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
